# Manipulating Large Language Models to Increase Product Visibility

This repository contains accompanying code for the paper titled [Manipulating Large Language Models to Increase Product Visibility](https://arxiv.org/abs/2404.07981).

## Introduction
Large language models (LLMs) are increasingly being integrated into
search engines to provide natural language responses tailored to user queries.
Customers and end-users are becoming more dependent on these models to make purchase
decisions and access new information. In this work, we investigate whether an LLM
can be manipulated to enhance the visibility of specific content or products in its
recommendations. We demonstrate that adding a strategic text sequence (STS)—a
carefully crafted message—to a product's information page or a website's content can
significantly increase its likelihood of being listed as the LLM's top recommendations.
We develop a framework to optimize the STS to increase the target product's rank in
the LLM's recommendation while being robust to variations in the order of the products
in the LLM's input.

To understand the impact of the strategic text sequences, we conduct empirical analyses
using datasets comprising catalogs of consumer products (such as coffee machines,
books, and cameras) and a collection of political articles. We measure the change in
visibility of a product or an article before and after the inclusion of the STS. We
observe that the STS significantly enhances the visibility of several products and articles
by increasing their chances of appearing as the LLM's top recommendation.
This ability to manipulate LLM-generated search responses provides vendors and
political entities with a considerable competitive advantage, posing potential risks to fair
market competition and the impartiality of public opinion.

The following figure shows the impact of adding an STS to a product's information page.
In the "Before" scenario, the target product is not mentioned in the LLM's recommendations.
However, in the "After" scenario, the STS on the product's information page enables the
target product to appear at the first position, improving its visibility in the LLM's recommendation.

<p align="center">
  <img src="figures/framework.png" width="500"/>
</p>

## This Repository

**Generating STS:** The file `rank_opt.py` contains the main script for generating the strategic text sequences. It uses the list of products in `data/coffee_machines.jsonl` as the catalog. It optimizes
the probability of the target product's rank being 1. 
Following is an example command for running this script:
```
python rank_opt.py --results_dir [path/to/save/results] --target_product_idx [num] --num_iter [num] --test_iter [num] --random_order --mode [self or transfer]
```

Options:

1. `--results_dir`: To specify the location to save the outputs of the script, such as the STS of the target product.

2. `--target_product_idx`: To specify the index of the target product in the list of products in `data/coffee_machines.jsonl`.

3. `--num_iter`: Number of iterations of the optimization algorithm.

4. `--test_iter`: Interval to test the STS.

5. `--random_order`: To optimize the STS to tolerate variations in the product order.

6. `--mode`: Mode in which to generate the STS:

    a. `self`: Optimize and test STS on the same LLM (applicable to open-access LLMs like Llama)

    b. `transfer`: Optimize to transfer to a different LLM (applicable for API-access models like GPT-3.5), e.g., Optimize using Llama and Vicuna, and test on GPT-3.5.

`rank_opt.py` generates the STS for the target product and plots the target loss and the rank of the target product in the results directory.
See `self.sh` and `transfer.sh` in `bash script` for usage of the above options.

`coffee_machines.jsonl` in `data` contains a catalog of ten fictitious coffee machines listed in increasing order of price.

**Evaluating STS:** `evaluate.py` evaluates the STS generated by `rank_opt.py`. We obtain product recommendations from an LLM with and without the STS in the target product's description in the catalog. We then compare the rank of the target product in the LLM's recommendation in the two scenarios. We repeat this experiment several times to quantify the advantage obtained from using the STS.
Following is an example command for running the evaluation script:
```
python evaluate.py --model_path [LLM for STS evaluation] --prod_idx [num] --sts_dir [path/to/STS] --num_iter [num] --prod_ord [random or fixed]
```

Options:

 1. `--model_path`: Path to the LLM to use for STS evaluation.

 2. `--prod_idx`: Target product index.

 3. `--sts_dir`: Path to STS to evaluate. Same as `--results_dir` for `rank_opt.py`.

 4. `--num_iter`: To specify the number of evaluations.

 5. `--prod_ord`: To specify the product order in the LLMs input.

 **Plotting Results:** `plot_dist.py` plots the distribution of the target product's rank before and after STS insertion. It also plots the advantage obtained by using the STS (% of times the target product ranks higher).

 See scripts `eval_self.sh` and `eval_transfer.sh` for usage of `evaluate.py` and `plot_dist.py`.

**System Requirements:** The strategic text sequences were optimized using NVIDIA A100 GPUs with 80GB memory. When run in transfer mode, `rank_opt.py` requires access to GPUs. All the abopve scripts need to be run in a Conda environment created as per the instructions below.

## Installation
Follow the instructions below to set up the environment for the experiments.

1. Install Anaconda:
    - Download .sh installer file from https://www.anaconda.com/products/distribution
    - Run: 
        ```
        bash Anaconda3-2023.03-Linux-x86_64.sh
        ```
2. Set up conda environment `llm-rank` with required packages:
    ```
    conda env create -f env.yml
    ```
3. Activate environment:
    ```
    conda activate llm-rank
    ```

### Manually Build Environment (Optional)
If setting up the environment using `env.yml` does not work, manually build an environment
with the required packages using the following steps:

1. Create Conda Environment with Python:
    ```
    conda create -n [env] python=3.10
    ```
2. Activate environment:
    ```
    conda activate [env]
    ```
3. Install PyTorch with CUDA from: https://pytorch.org/
	```
    conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia
    ```
4. Install transformers from Huggingface:
    ```
    pip install transformers
    ```
    <!-- conda install -c huggingface transformers -->
5. Install accelerate:
    ```
    conda install -c conda-forge accelerate
    ```
<!-- 6. Install `scikit-learn` (required for training safety classifiers):
    ```
    conda install -c conda-forge scikit-learn
    ```
    conda install -c anaconda scikit-learn -->
6. Install `seaborn`:
    ```
    conda install anaconda::seaborn
    ```
7. Install `termcolor`:
    ```
    conda install -c conda-forge termcolor
    ```
8. Install OpenAI python package:
    ```
    conda install conda-forge::openai
    ```

9. Install Anthropic python package: 
    ```
    pip install anthropic
    ```